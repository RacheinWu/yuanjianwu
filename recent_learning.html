<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Recent Learning</title>
</head>
<body style="margin: 0;padding: 0">
<div style="background-color: #ffffff;border: 1px solid gray;padding: 0 20px 0 20px ">
    <div>
        <h3><span>#1</span>
            <span style="color: rgb(1,120,55)">ReplaceMe: Network Simplification via Layer Pruning and Linear Transformations</span>
            <span style="color: rgb(236,101,0)">[Notion] [Zhihu] [Bilibili]</span>
        </h3>
    </div>
    <div>
        We introduce ReplaceMe linear operatito conventional prun Our experrinvolve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original model's performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead (see Fig.1). We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at this repository.
    </div>
    <div>
        <h4>Subject: <span style="color: rgb(236,101,0)">LLMs</span>, <span style="color: rgb(236,101,0)">Few-shot Learning</span></h4>
    </div>
    <div>
        <h4>Updated-time: <span style="font-weight: normal">2025-05-06</span></h4>
    </div>
</div>
</body>
</html>